# Modelagem de Séries Temporais e Real-Time Analytics com Apache Spark e Databricks
# Instalação e Configuração do Cluster Spark

# Abra o terminal ou prompt de comando e acesse a pasta onde estão os arquivos no seu computador

# Execute o comando abaixo para criar e inicializar o Cluster
docker compose -f docker-compose.yml up -d --scale spark-worker-dsa=2

# Execute os scripts dentro do container
python 01-dsa_gera_dados_treino.py
python 02-dsa-treina-modelo.py
python 03-deploy.py

# Spark Master
http://localhost:9090

# History Server
http://localhost:18080

# Para desligar o Cluster
docker compose -f docker-compose.yml down